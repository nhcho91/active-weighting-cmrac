\documentclass[nobib]{my-handout}

\date{August 21, 2018}

% \newtheorem{theorem}{Theorem}
% \newtheorem{corollary}{Corollary}
% \newtheorem{lemma}{Lemma}

% \theoremstyle{definition}
% \newtheorem{definition}{Definition}
% \newtheorem{assumption}{Assumption}
% \newtheorem{algorithm}{Algorithm}

% \theoremstyle{remark}
\newtheorem{observation}{Observation}
% \newtheorem{remark}{Remark}

\DeclareMathOperator{\gap}{gap}
\DeclareMathOperator{\Gap}{Gap}
\DeclareMathOperator{\diag}{diag}
\newcommand{\lmin}{\lambda_{\min}}
\newcommand{\lmax}{\lambda_{\max}}

\begin{document}

\maketitle


\section{Introduction}

Composite adaptive control combines \textit{direct} and
\textit{indirect} schemes of adaptive
control~\cite{lavretsky_combined/composite_2009}. The purpose is
\begin{itemize}
	\item to obtain the global asymptotic stability of \textit{both} tracking
		and parameter estimation errors with proper exciting conditions and a
		matching condition, or
	\item to simply improve the tracking performance, where the matching condition
		or the exciting conditions are not satisfied.
\end{itemize}

\newthought{Fundamental idea} is to exploit a current estimation of the
parameter estimation error. Consider\sidenote{We define $\Delta(t) = {W^\ast}^T
\phi(t) + \varepsilon(t)$.}
\begin{equation*}
	\dot{e}(t) = A e(t) + B \qty(u(t) + \Delta(t)).
\end{equation*}
Convert this system into\sidenote{Sometimes we filter the system as
\begin{equation*}
	y_f = {W^\ast}^T \phi_f(t) + \varepsilon_f(t),
\end{equation*}}
\begin{equation}\label{eq:regression_form}
	y(t) = {W^\ast}^T \phi(t) + \varepsilon(t),
\end{equation}
where $y(t)$ is measured using
\begin{equation*}
	y(t) = B^\dagger \qty(\dot{e}(t) - Ae(t)) - u(t).
\end{equation*}

\begin{observation}
	\begin{enumerate}
		\item[]
		\item equation \eqref{eq:regression_form} is merely a linear regression form,
			and
		\item almost all composite adaptive control
			schemes~\cite{lavretsky_combined/composite_2009, cho_composite_2018,
			chowdhary_exponential_2014} use the update law of standard least square
			regression, which are represented by
			\begin{equation}\label{eq:general_form_of_composite_update_law}
				\dot{W}(t) = \Gamma_1 \phi(t) e^T P B - \int_0^t c(t, \tau) \phi(\tau)
				\epsilon^T(t, \tau) \dd{\tau},
			\end{equation}
			where
			\begin{equation*}
				\epsilon(t, \tau) = W^T(t) \phi(\tau) - y(\tau).
			\end{equation*}
	\end{enumerate}
\end{observation}

\begin{observation}
	Consider $c(t, \tau)$ in
	equation~\eqref{eq:general_form_of_composite_update_law}, 
	\begin{enumerate}
		\item \textbf{Standard Least Square Update~\cite{slotine_applied_1991}:} If
			$c(t, \tau) = \delta(t-\tau)$ where $\delta$ is a Dirac delta function,
			then the update law is a standard least square form, which requires the
			PE condition for exponential convergence.
		\item \textbf{Concurrent Learning~\cite{chowdhary_exponential_2014}:} If
			$c(t, \tau) = \sum_{i=1}^p \delta(t_i - \tau)$ for $0 \le t_i \le t$,
			then the update law is a concurrent learning form, which requires the
			exciting over finite interval condition.
		\item \textbf{Y. Pan~\cite{pan_composite_2018} and N.
			Cho~\cite{cho_composite_2018}:} If $c(t, \tau) = \exp(- \int_\tau^{t_i}
			k(\nu) \dd{\nu})$ for $t_0 \le t_i \le t$, then the update law is the
			form suggested in, which requires the IE or FE condition.
	\end{enumerate}
\end{observation}


\section{Motivation}

\begin{itemize}
	\item \textbf{Without the PE Condition:} The standard least square update is
		valid only with the PE condition.
	\item \textbf{Time-Varying Parameters:} Concurrent learning, Y. Pan and N.
		Cho's algorithms are not suited for time-varying parameter estimation, as
		it can be stuck in the past time where the minimum singular/eigenvalue are
		dominant.
	\item \textbf{Stochastic Estimation:} The standard least square update can
		deal with the stochastic estimation\sidenote{$\varepsilon(t)$ is a random
		variable} only when the PE condition is satisfied. Concurrent learning and
		its variations are sensitive to such noises, as the history stack
		algorithms are heavily dependent on the singular values.
	\item \textbf{Smooth Estimation}: Parameter estimation in concurrent
		learning, Y. Pan and N. Cho's algorithms are not smooth, as the update is
		piecewise constant in time.
\end{itemize}


\section{Preliminaries}

\begin{theorem}[Weyl, see~\cite{horn_matrix_2012}]
	Let $A$ and $B$ be n-by-n Hermitian matrix and let the respective eigenvalues
	of $A$, $B$, and $A+B$ be $\{\lambda_i(A)\}_{i=1}^n$,
	$\{\lambda_i(B)\}_{i=1}^n$, and $\{\lambda_i(A+B)\}_{i=1}^n$, ordered
	algebraically as $\lambda_{\max} = \lambda_n \ge \lambda_{n-1} \ge \cdots \ge
	\lambda_2 \ge \lambda_1 = \lambda_{\min}$. Then,
	\begin{equation*}
		\lambda_i(A+B) \le \lambda_{i+j}(A) + \lambda_{n-j}(B)\qc j=0,1,\dotsc,n-i
	\end{equation*}
	for each $i=1,\dotsc,n$. Also,
	\begin{equation*}
		\lambda_{i-j+1}(A) + \lambda_j(B) \le \lambda_i(A+B)\qc j=1,\dotsc,i
	\end{equation*}
	for each $i=1,\dotsc,n$.
\end{theorem}

\begin{definition}[Gap, see~\cite{ipsen_refined_2009}]
	Let $A$ be n-by-n matrix and let the eigenvalues of $A$ be
	$\{\lambda_i\}_{i=1}^n$. The \textit{gap} is defined as 
	\begin{equation*}
		\gap_{i}(A) = \lambda_{i+1}(A) - \lambda_i(A),
	\end{equation*}
	for $i \in \{1, \dotsc, n-1\}$, and
	\begin{equation*}
		\gap_n(A) = \lambda_n(A) - \lambda_{n-1}(A).
	\end{equation*}
\end{definition}


\section{Problem Formulation}

In \eqref{eq:general_form_of_composite_update_law}, let
\begin{equation*}
	\MC{A}(t) = \int_0^t c(t, \tau) \phi(\tau) \phi^T(\tau) \dd{\tau},
\end{equation*}
whose derivative is
\begin{equation*}
	\dot{\MC{A}}(t) = \int_0^t \pdv{t} c(t, \tau) \phi(\tau) \phi^T(\tau)
	\dd{\tau} + c(t, t) \phi(t) \phi^T(t) \qc \MC{A}(0) = 0.
\end{equation*}
To deliberately consider a forgetting factor, let $c(t, \tau)$ be
\begin{equation*}
	c(t, \tau) = \beta(\tau) \exp(- \int_\tau^t \alpha(\nu) \dd{\nu}),
\end{equation*}
for $\alpha: \MB{R}_{+} \to \MB{R}$ and $\beta: \MB{R}_{+} \to \MB{R}$,
which leads to 
\begin{equation*}
	\dot{\MC{A}}(t) = - \alpha(t) \MC{A}(t) + \beta(t) \phi(t) \phi^T(t) \qc
	\MC{A}(0) = 0.
\end{equation*}

Note that there are no constraints for $\alpha$ and $\beta$, if they guarantee
the BIBO stability.\sidenote{Need references}

With slight abuse of notations for simplicity, the discrete counterpart can be
written as
\begin{equation}\label{eq:discrete_update_law}
	A^{k+1} = a_k A^k + b_k v_k v_k^T \qc A^0 = 0,
\end{equation}
for $a_k > 0,\ b_k \in \MB{R}$, where $A^k \coloneqq \MC{A}(k \Delta t)$, and $v_k
\coloneqq \phi(k \Delta t)$.

\newthought{The purpose} is to design $a_k$ and $b_k$
\begin{enumerate}
	\item to increase the minimum eigenvalue of $A^k$ as $k$ increases, and
	\item to bound, simultaneously, the maximum eigenvalue of $A^k$.
\end{enumerate}
for given $v_k$ at each step $k$.

\begin{theorem}[Ipsen, see~\cite{ipsen_refined_2009}]
	\label{thm:ipsen}
	Let $A \in \MB{C}^{n \times n}$ be Hermitian, $y \in \MB{C}^n$.
	\begin{enumerate}
		\item (smallest eigenvalue). Let
			\begin{align*}
				L_{\pm} &\coloneqq \mqty(\lambda_2(A) & 0 \\ 0 & \lambda_1(A)) \pm
				\mqty(\norm{y_{2:n}} \\ y_1) \mqty(\norm{y_{2:n}} & \bar{y}_1), \\
				U_{\pm} &\coloneqq \mqty(\lambda_2(A) & 0 \\ 0 & \lambda_1(A)) \pm
				\mqty(y_2 \\ y_1) \mqty(\bar{y}_2 & \bar{y}_1).
			\end{align*}
			Then
			\begin{equation*}
				\lmin(L_{\pm}) \le \lmin(A \pm y y^\ast) \le \lmin(U_{\pm}),
			\end{equation*}
			where
			\begin{gather*}
				\lmin(A) \le \lmin(L_{+}) \le \lmin(U_{+}) \le \lambda_2(A), \\
				\lmin(A) - \norm{y}^2 \le \lmin(L_{-}) \le \lmin(U_{-}) \le \lmin(A).
			\end{gather*}
		\item (largest eigenvalue). Let
			\begin{align*}
				L_{\pm} &\coloneqq \mqty(\lambda_n(A) & 0 \\ 0 & \lambda_{n-1}(A)) \pm
				\mqty(\norm{y_n} \\ y_{n-1}) \mqty(\bar{y}_n & \bar{y}_{n-1}), \\
				U_{\pm} &\coloneqq \mqty(\lambda_n(A) & 0 \\ 0 & \lambda_{n-1}(A)) \pm
				\mqty(y_n \\ \norm{y_{1:n-1}}) \mqty(\bar{y}_n & \norm{y_{1:n-1}}).
			\end{align*}
			Then
			\begin{equation*}
				\lmax(L_{\pm}) \le \lmax(A \pm y y^\ast) \le \lmax(U_{\pm}),
			\end{equation*}
			where
			\begin{gather*}
				\lmax(A) \le \lmax(L_{+}) \le \lmax(U_{+}) \le \lmax(A)+\norm{y}^2, \\
				\lambda_{n-1}(A) \le \lmax(L_{-}) \le \lmax(U_{-}) \le \lmax(A).
			\end{gather*}
	\end{enumerate}
\end{theorem}


\section{Main Results}

Let $A$ be an n-by-n positive semidefinite matrix, and $v$ be an
n-dimensional real vector, and $\{\lambda_i(\cdot)\}$ be the eigenvalues
of $(\cdot)$ ordered algebraically as $\lambda_{\max} = \lambda_n \ge
\lambda_{n-1} \ge \cdots \ge \lambda_2 \ge \lambda_1 = \lambda_{\min}$.

Let $\Lambda$ and $V$ be the diagonal matrix of eigenvalues of $A$, and the
corresponding matrix of eigenvectors, i.e.
\begin{equation*}
	A = V \Lambda V^T.
\end{equation*}
Also, let
\begin{equation*}
	A^\prime = a A + b v v^T,
\end{equation*}
which is an abbreviated form of~\eqref{eq:discrete_update_law}.
	
Now, we want to derive an algorithm that
\begin{enumerate}
	\item increases the minimum eigenvalue,
		\begin{equation*}
			\lambda_1 (A^\prime) \ge \lambda_1 (A),
		\end{equation*}
	\item and bounding the maximum eigenvalue as
		\begin{equation*}
			\lambda_{\max}(A^\prime) \le \lambda_{\max}(A).
		\end{equation*}
\end{enumerate}


\section{Main Results}

\begin{lemma}\label{lem:ipsen_eigenvalue_bounds}
	Let $A \in \MB{C}^{n \times n}$ be Hermitian, $v \in \MB{C}^n$, and $A^\prime
	\coloneqq a A + b v v^T$ for $a > 0$ and $ b \in \MB{R}$. Then,
	\begin{fullwidth}
		\begin{equation}
			\lambda_{\min}(A^\prime)
			\ge a \lambda_1(A) + \frac{1}{2} \qty(a \gap_1(A) + b \norm{v}^2 -
			\sqrt{\qty(a \gap_1(A) + b \norm{v}^2)^2 - 4 a b \gap_1(A) \abs{v_1}^2}),
		\end{equation}
		and
		\begin{equation}
			\lambda_{\max}(A^\prime)
			\le a \lambda_n(A) + \frac{1}{2} \qty(- a \gap_n(A) + b \norm{v}^2 
			+ \sqrt{\qty(a \gap_n(A) + b \norm{v}^2)^2 - 4 a b \gap_n(A)
			\abs{v_{1:n-1}}^2}),
		\end{equation}
	\end{fullwidth}
	\end{lemma}

\begin{proof}
	With the fact that $\lambda_i(a A) = a \lambda_i(A)$, and $\gap_i(a A) = a
	\gap_i(A)$, the proof directly follows Theorem 2.1, and Corollary 2.2
	of~\cite{ipsen_refined_2009}.
\end{proof}

For simplicity, we abbreviate $\lambda_i \coloneqq \lambda_i(A)$, and $\gap_i
\coloneqq \gap_i(A)$.  Define a function $f : \MB{R}_{+} \times
\MB{R}_{+} \to \MB{R}_{+}$, such that
\begin{equation}\label{eq:function_of_lmin_lbound}
	f(a,b) \coloneqq
	a \lambda_1 + \frac{1}{2} \qty(a \gap_1 + b \norm{v}^2 - \sqrt{\qty(a
	\gap_1 + b \norm{v}^2)^2 - 4 ab \gap_1 \abs{v_1}^2}).
\end{equation}

\begin{lemma}
	$f(a, b)$ is a monotonically increasing function for each $a > 0$ and $b
	\in \MB{R}$.

	Moreover, given $a$,
	\begin{equation*}
		\lim_{b \to \infty} f(a, b) = a \qty(\lambda_1(A) + \gap_1(A)
		\frac{\abs{v_1}^2}{\norm{v}^2}).
	\end{equation*}
\end{lemma}

\begin{lemma}
	Suppose that there exist $a, b \ge 0$, such that $f(a, b) \ge
	\lambda_1(A)$. Then
	\begin{equation}\label{eq:neccessary_cond_for_a}
		a \ge \frac{\lambda_1(A)}{\lambda_2(A)}.
	\end{equation}
\end{lemma}

Given $A \in \MB{S}^{n \times n}$, and $v \in \MB{R}^n$, let $f_1 :
\MB{R}_{+} \times \MB{R} \times \MB{R} \to \MB{R}$ be
\begin{equation*}
	f_1(a, b; c) \coloneqq a^2 + \frac{k_1 \norm{v}^2}{\lambda_1} a b -
	\frac{\lambda_1 + \lambda_2}{\lambda_1 \lambda_2} a c -
	\frac{\norm{v}^2}{\lambda_1 \lambda_2} b c +
	\frac{1}{\lambda_1 \lambda_2} c^2,
\end{equation*}
where
\begin{gather*}
	a + \frac{\norm{v}^2}{\lambda_1 + \lambda_2} b - \frac{2 c}{\lambda_1
	+ \lambda_2} \ge 0,\\
	\frac{\lambda_1}{\lambda_2} \le k_1 \coloneqq \frac{\lambda_1}{\lambda_2} +
	\frac{\abs{v_1}^2}{\norm{v}^2} \qty(1 - \frac{\lambda_1}{\lambda_2}) \le 1.
\end{gather*}
Also, let $f_n : \MB{R}_{+} \times \MB{R} \times \MB{R} \to \MB{R}$
\begin{equation*}
	f_n(a, b; c) \coloneqq a^2 + \frac{k_n \norm{v}^2}{\lambda_n} a b -
	\frac{\lambda_n + \lambda_{n-1}}{\lambda_n \lambda_{n-1}} a c -
	\frac{\norm{v}^2}{\lambda_n \lambda_{n-1}} b c + \frac{1}{\lambda_n
	\lambda_{n-1}} c^2,
\end{equation*}
where
\begin{gather*}
	a + \frac{\norm{v}^2}{\lambda_n + \lambda_{n-1}} b - \frac{2 c}{\lambda_n +
	\lambda_{n-1}} \le 0, \\
	1 \le k_n \coloneqq \frac{\lambda_n}{\lambda_{n-1}} -
	\frac{\abs{v_n}^2}{\norm{v}^2} \qty(\frac{\lambda_n}{\lambda_{n-1}} - 1) \le
	\frac{\lambda_n}{\lambda_{n-1}}.
\end{gather*}

\begin{theorem}
	\begin{enumerate}
		\item Suppose that there exist $a>0, b, c \in \MB{R}$ satisfying
			$f_1(a, b; c) \ge 0$. Then, $\lambda_{\min}(a A + b v v^T) \ge c$.
		\item Also, if $f_n(a, b; c) \ge 0$, then, $\lambda_{\max}(a A + b v
			v^T) \le c$.
		\end{enumerate}
\end{theorem}

\begin{proof}
	The proof is direct result from Lemma~\ref{lem:ipsen_eigenvalue_bounds},
\end{proof}

\begin{remark}
	\begin{enumerate}
		\item If $\gap_1 = 0$, then $k_1 = 1$, and $f_1(a, b, c) \ge 0$ reads
			\begin{gather*}
				f_1(a, b; c) = \qty(a - \frac{c}{\lambda_1}) \qty(a +
				\frac{\norm{v}^2}{\lambda_1} b - \frac{c}{\lambda_1}) \ge 0, \\
				a + \frac{\norm{v}^2}{2 \lambda_1} b - \frac{c}{\lambda_1} \ge 0.
			\end{gather*}
		\item If $\gap_n = 0$, then $k_n = 1$, and $f_n(a, b, c) \ge 0$ reads
			\begin{gather*}
				f_n(a, b; c) = \qty(a - \frac{c}{\lambda_n}) \qty(a +
				\frac{\norm{v}^2}{\lambda_n} b - \frac{c}{\lambda_n}) \ge 0, \\
				a + \frac{\norm{v}^2}{2 \lambda_n} b - \frac{c}{\lambda_n} \le 0.
			\end{gather*}
	\end{enumerate}
\end{remark}

\begin{remark}
	Note that $f_1(c_1/\lambda_1, 0; c_1) = f_n(c_n/\lambda_n, 0; c_n) = 0$ for
	all $c_1, c_2 \in \MB{R}$.
\end{remark}

\begin{lemma}
	Let $A \in \MB{S}^{n \times n}$ and $v \in \MB{R}^n$. Then, for all $c_1, c_n
	\in \MB{R}$ such that
	\begin{equation*}
		\frac{c_1}{\lambda_1} \le \frac{c_n}{\lambda_n},
	\end{equation*}
	there exist $a>0, b \in \MB{R}$ satisfying $c_1 \le \lambda_i(a A + b v v^T)
	\le c_2$, for $i = 1, \dotsc, n$.
\end{lemma}

\begin{theorem}
	Let $A \in \MB{S}^{n \times n}$ and $v \in \MB{R}^n$. Suppose that 
	\begin{gather}
		\frac{\abs{v_1}^2}{\lambda_{\min}(A)} \ne
		\frac{\abs{v_n}^2}{\lambda_{\max}(A)}, \\
		\gap_1, \gap_n \ne 0.
	\end{gather}
	Then, there exist $a > 0$ and $b \in \MB{R}$ such that
	\begin{equation*}
		c_1 < \lambda_i(a A + b v v^T) < c_n,
	\end{equation*}
	for all $c_1, c_2$ such that $c_1/\lambda_1 \le c_n/\lambda_n$.
\end{theorem}

\begin{proof}
	Note that $f_1(c_1/\lambda_1, 0; c_1) = f_n(c_n/\lambda_n, 0; c_n) = 0$, and
	\begin{align*}
		\grad{f_1}(c_1/\lambda_1, 0; c_1) &= \frac{c_1 \gap_1}{\lambda_1 \lambda_2}
		\mqty[1 & \frac{\abs{v_1}^2}{\lambda_1}]^T, \\
		\grad{f_n}(c_n/\lambda_n, 0; c_n) &= - \frac{c_n \gap_n}{\lambda_n
		\lambda_{n-1}} \mqty[1 &
		\frac{\abs{v_n}^2}{\lambda_n}]^T.
	\end{align*}
	Since $f_1(a, b; c_1) = 0$ and $f_n(a, b; c_n) = 0$ intersect the point
	$(a,b) = (c_1/\lambda_1, 0)$, and $(a, b) = (c_n/\lambda_n, 0)$,
	respectively, there exists a region $\MC{D} \subset \MB{R}_{+} \times
	\MB{R}$ satisfying both $f_1(a, b; c_1) > 0$ and $f_n(a, b; c_n) > 0$ for
	$(a,b) \in \MC{D}$, if only 
	\begin{equation*}
		\grad{f_1}(c_1/\lambda_1, 0; c_1) \times \grad{f_n}(c_n/\lambda_n, 0; c_n)
		\ne 0,
	\end{equation*}
	which completes the proof.
\end{proof}


\section{Simulations}

We consider the following system
\begin{equation*}
	\dot{x}(t) = \mqty[0 & 1 & 0 \\ -15.8 & -5.6 & -17.3 \\ 1 & 0 & 0] x(t) +
	\mqty[0 \\ 0 \\ -1] c(t) + \mqty[0 \\ 1 \\ 0] \qty(u(t) + \Delta(x)),
\end{equation*}
where ${W^\ast}^T \phi(x)$, for $W^\ast \coloneqq [-18.59521, 15.162375,
-62.45153, 9.54708, 21.45291]^T$ and $\phi(x) \coloneqq [x_1, x_2, \abs{x_1}
x_2, \abs{x_2} x_2, x_1^3]^T$. The command signal $c(t)$ is given by a square
signal.

The reference model is
\begin{equation*}
	\dot{x}_r(t) = \mqty[0 & 1 & 0 \\ -15.8 & -5.6 & -17.3 \\ 1 & 0 & 0]
	x_r(t) + \mqty[0 \\ 0 \\ -1] c(t).
\end{equation*}

% \begin{table}
% 	\centering
% 	\caption{MRAC Parameters} 
% 	\begin{tabular}{@{}lrlr@{}} \toprule
% 		Parameter & Value & Parameter & Value \\ \midrule
% 		$\Gamma_1$ & $10^4$ & $Q$ & $I_3$ \\
% 		$\tau_f$ & $10^{-3}$ & $\lambda_0$ & $10^{-8}$ \\ \bottomrule
% 	\end{tabular}
% \end{table}


\section{Results}

\begin{figure*}[h]
	\includegraphics[width=\textwidth]{state_input.png}
	\caption{This figure shows states and inputs histories.}
\end{figure*}


\begin{figure}[h]
	\includegraphics[width=\textwidth]{parameter_estimation_normed.png}
	\caption{This figure shows normed parameter estimation errors.}
\end{figure}

\begin{marginfigure}[1cm]
	\includegraphics[width=\textwidth]{parameter_estimation.png}
	\caption{This figure shows element-wise parameter estimation history.}
\end{marginfigure}

\begin{figure}[h]
	\includegraphics[width=\textwidth]{eigenvalues.png}
	\caption{This figure shows the bounds of eigenvalues.}
\end{figure}

\begin{figure*}[h]
	\includegraphics[width=\textwidth]{a_and_b.png}
	\caption{This figure shows the history of $a$ and $b$.}
\end{figure*}

\bibliographystyle{plain}
\bibliography{../global.bib}

\end{document}
