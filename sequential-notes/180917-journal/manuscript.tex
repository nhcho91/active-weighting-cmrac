\documentclass[]{IEEEtran}

% Packages
% ========
\usepackage{cite}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{algorithmicx}
\usepackage{physics}


% Recommend settings
% ==================
\interdisplaylinepenalty=2500  % (from bare_jnrl.tex of IEEEtran} 

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{corollary}{Corollary}
\newtheorem{proposition}{Proposition}

\theoremstyle{definition}
\newtheorem{definition}{Definition}
\newtheorem{problem}{Problem}
\newtheorem{example}{Example}
\newtheorem{algorithm}{Algorithm}
\newtheorem{assumption}{Assumption}

\theoremstyle{remark}
\newtheorem{remark}{Remark}
\newtheorem{note}{Note}


% Macros
% ======
\newcommand{\MB}[1]{\mathbb{#1}}
\newcommand{\MC}[1]{\mathcal{#1}}
\newcommand{\tsup}[1]{\textsuperscript{#1}}
\newcommand{\tsub}[1]{\textsubscript{#1}}
\DeclareMathOperator{\diag}{diag}


\begin{document}

\title{Data-Efficient Composite Adaptive Control}
\author{Seonng-hun Kim, Namhoon Kim, Youdan Kim}
\author{Seong-hun~Kim,~\IEEEmembership{Member,~IEEE,}
        Namhoon~Cho,~\IEEEmembership{Member,~IEEE,}
        and~Youdan~Kim,~\IEEEmembership{Member,~IEEE} % <-this % stops a space
\thanks{S-h. Kim, N. Cho, and Y. Kim are with the Department
of Mechanical and Aerospace Engineering, Seoul National University, Seoul,
30332 Republic of Korea e-mail: bgbgof@snu.ac.kr.}}
\maketitle
% ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~



% Abstract
% ========
\begin{abstract}
	We propose...
\end{abstract}
% ~~~~~~~~~~~~


% Keywords
% ========
\begin{IEEEkeywords}
	Composite adaptive control, rank-one update
\end{IEEEkeywords}
% ~~~~~~~~~~~~~~~~


\section{Introduction}
% ====================

Model-based control is well-established design scheme for class of systems whose dynamics are perfectly known.
However, it is impossible to know the perfect system dynamics, especially in the real-world applications.
Systems may be faced with some uncertainties caused by modelling errors, dynamic environments, or even changes in the given mission.
A small amount of uncertain signal can efficiently be suppressed by the inherent robustness of the model-based controllers.
When the uncertainties get larger, it can make the system deviate from the set point or even unstable.
With the need for higher levels of autonomy to address these issues, studies of \textit{adaptive} control techniques have been actively conducted~\cite{astrom_adaptive_1987}.

An uncertanty is usually treated as an unexpected signal inserted into the known system.
If the uncertain signal can be parameterized with known basis functions, the first thing we can try is to estimate the parameters.
Any controller can be synthesized based on the estimated system, called \textit{indirect} adaptive control.
However, it is hard to guarantee the stability during the learning phase, as the controller strictly depends on the estimation.
Another approach, \textit{direct} adaptive control scheme, is aimed at stabilizing the system without perfect estimation of the uncertainty.




Recently, there has been a great amount of work to exploit the computing power
to improve the performance of classical control frameworks. For example, an
inverse model or a forward model of nonlinear systems is identified using a set
of measurements, then the control parameter is obtained in an optimal sense
based on that models~\cite{novara_data-driven_2018}.

Most of these data-based approaches aim at estimating an unknown model of the
system using dataset. However, the dataset obtained by experiments based on the
dynamic system is usually not rich enough to fully identify the system. The
richness is represented by the persistent excitation (PE) condition in the
adaptive control literature. So called the composite adaptive control (CAC)
of which the purpose is to identify the uncertain system while maintaining the
nominal performance.

There is a lot of algorithms proving the exponential convergence, under each
excitation condition.

In this paper, we propose the generalization of such excitation conditions, and
how to ensure that excitation condition is met.

\subsection{Organization of paper}
% --------------------------------

This paper is organized as follows.


\section{Preliminaries}
% =====================

Consider the following linearly parameterized system
\begin{equation*}
	y(t) = {W^\ast}^T \phi(t),
\end{equation*}
where $W^\ast \in \MB{R}^{n \times m}$ is the unknown, constant parameter to
estimate, $\phi: \MB{R}_{+} \to \MB{R}^n$ and $y(t) \in \MB{R}^m$ are the
corresponding basis function and the output vector. An estimator widely used
for this system is the least-square estimator given by
\begin{equation}\label{eq:estimation system}
	\dot{W}(t) =  - \Gamma \phi(t) \qty(\phi^T(t) W(t) - y^T(t)).
\end{equation}
where $\Gamma$ is a positive-definite matrix denoting the update rate. Let the
estimation error be $\tilde{W}(t) \coloneqq W(t) - W^\ast$. Then the error
dynamics can be written as
\begin{equation}\label{eq:estimation error system}
	\dot{\tilde{W}} (t) = - \Gamma \phi(t) \phi^T(t) \tilde{W}(t)
\end{equation}

The global, exponential stability of the
system~\eqref{eq:estimation error system} to zero is guaranteed under the
following persistent excitation condition~\cite{anderson_exponential_1977,
slotine_applied_1991}.

\begin{definition}[Persistent Excitation (PE)]
	A signal $\phi: \MB{R}_{+} \to \MB{R}^n$ is \textit{persistently exciting
	(PE)} if there exist positive scalars $\beta_1$, $\beta_2$, and $\delta$ such
	that
	\begin{equation}\label{eq:PE}
		\beta_1 I \le \int_{t_0}^{t_0+\delta} \phi(\tau) \phi^T(\tau) \dd{\tau} \le
		\beta_2 I,
	\end{equation}
	for all $t_0 \ge 0$.
\end{definition}


% \section{Data-Based Composite Adaptive Control}
% % ====================================================

% In this section, we suggest the generalized PE condition, which can cover many
% variations of the composite adaptive control. Up to now, lots of
% algorithms have been developed, which perform perfectly both in command
% tracking and parameter estimation. However, each algorithm has its own
% excitation condition, e.g. relaxed PE condition for standard least-squares
% estimators~\cite{slotine_applied_1991}, the rank condition for
% concurrent-learning-based estimators~\cite{chowdhary_exponential_2014}, and so
% on. To standardize the performances or to see what are the differences between
% those algorithms, 

\section{Data Exploit Estimator}
% =============================

In this section, we first formulate an estimator which
generalizes~\eqref{eq:estimation system} with an unlimited memory that can
store all the previous data.  Then, we drive the corresponding excitation
condition by which the global exponential stability is guaranteed.  In
addition, we will show that lots of reported online parameter estimators can be
classified into a special case of this estimator.

Being equipped with the unlimited memory, the estimator \eqref{eq:estimation
system} can be naturally expanded to exploit the past data as follows
\begin{equation}\label{eq:estimation system memory}
	\dot{W}(t) = - \int_0^t d(t, \tau) \phi(\tau) \qty(\phi^T(\tau) W(t) -
	y^T(\tau)) \dd{\tau},
\end{equation}
with $d(t, \tau): \MB{R}_{+} \times \MB{R}_{+} \to \MB{R}$ being a function
that defines the time-varying distribution on $T \coloneqq [0, \infty]$.

Note that the vectors $y(\tau)$ and $\phi(\tau)$ are measured at time $\tau$ and
stored in the memory, and called consistently at the update time $t \ge \tau$.
The estimator~\eqref{eq:estimation system memory} is analogous to batch
gradient descent laws of stochastic optimization, except that there is a
distribution of the signal $\phi(\tau) \phi^T(\tau)$.

Consider the following error dynamics corresponding to~\eqref{eq:estimation
system memory}.
\begin{equation}\label{eq:estimation error system memory}
	\dot{\tilde{W}}(t) = - \int_0^t d(t, \tau) \phi(\tau) \phi^T(\tau) \dd{\tau}
	\tilde{W}(t).
\end{equation}

We now introduce an excitation condition of a signal that guarantees the
exponential stability of~\eqref{eq:estimation error system memory}

\begin{definition}[Persistent Excitation and Exploitation
	(PE\tsup{2})]
	A signal $\phi: \MB{R}_{+} \to \MB{R}^n$ is \textit{persistently exciting and
	exploited (PE\tsup{2})} if there exist positive scalars $\beta_1$, $\beta_2$,
	$\delta$ and a real-valued function $d : \MB{R}_{+} \times \MB{R}_{+} \to
	\MB{R}$ such that
	\begin{equation}\label{eq:PE2}
		\beta_1 I \le \int_{t_0}^{t_0+\delta} \int_{0}^s d(s, \tau) \phi(\tau)
		\phi^T(\tau) \dd{\tau} \dd{s} \le \beta_2 I,
	\end{equation}
	for all $t_0 \ge 0$.
\end{definition}

Note that PE\tsup{2}
We can now state the analogue of~\cite[Theorem~1]{anderson_exponential_1977}.

\begin{theorem}\label{thm:PE2 then GES}
	If $\phi: \MB{R}_{+} \to \MB{R}^n$ is PE\tsup{2}, then the
	system~\eqref{eq:estimation error system memory} is globally exponentially
	stable.
\end{theorem}

\begin{proof}
	Since $d \ge0 $, there is a unique matrix-valued function $\MC{H} :
	\MB{R}_{+} \to \MB{R}^{n \times n}$ such that
	\begin{equation*}
		\int_0^s d(s, \tau) \phi(\tau) \phi(\tau)^T \dd{\tau} = \MC{H}(s)
		\MC{H}(s)^T.
	\end{equation*}
	Now, the rest of proof is identical
	to~\cite[Theorem~1]{anderson_exponential_1977} except that the PE signal is
	matrix-valued as
	\begin{equation*}
		\beta_1 I \le \int_{t_0}^{t_0 + \delta} \MC{H}(s) \MC{H}^T(s) \dd{s}
		\le \beta_2 I.
	\end{equation*}

	\begin{gather*}
		\int_0^{t_0}{\int_{t_0}^{t_0 + \delta}{d(s, \tau) \phi(\tau) \phi^T(\tau)
		\dd{s}}\dd{\tau}}\\
		+ \int_{t_0}^{t_0 + \delta}{\int_{\tau}^{t_0 + \delta}{d(s, \tau)
		\phi(\tau) \phi^T(\tau) \dd{s}}\dd{\tau}} \\
		= \int_0^{t_0}{\qty(D(t_0 + \delta, \tau) - D(t_0, \tau)) \phi(\tau)
		\phi^T(\tau) \dd{\tau}} \\
		+ \int_{t_0}^{t_0 + \delta}{\qty(D(t_0 + \delta, \tau) - D(\tau, \tau))
		\phi(\tau) \phi^T(\tau) \dd{\tau}}
	\end{gather*}
\end{proof}

Note that the proposed definition of PE\tsup{2} is a generalized version of
various excitation conditions, including memory-equipped
estimators~\cite{chowdhary_exponential_2014, pan_composite_2018,
	cho_composite_2018} or memoryless estimators~\cite{anderson_exponential_1977,
slotine_composite_1989}.

\begin{example}[Gradient Estimators]
	\label{ex:gradient estimators}
	Suppose that $d(s, \tau) = \gamma \delta_D(s -
	\tau)$ where $\delta_D : \MB{R} \to \MB{R}_{+}$ is the Dirac's delta function
	and with $\gamma > 0$. Then the PE\tsup{2} condition becomes the PE condition,
	and the corresponding estimator~\eqref{eq:estimation system memory}
	becomes~\eqref{eq:estimation system} where $\Gamma = \gamma I$.
\end{example}

Example~\ref{ex:gradient estimators} shows that the memoryless estimators, e.g.
the gradient estimators and the least-squares estimators, are of one specific
form of~\eqref{eq:estimation system memory} that only use most recent data.

However, the PE\tsup{2} condition~\eqref{eq:PE2}, as well as the PE
condition~\eqref{eq:PE}, is hard to check online since it needs to find
$\delta$. Hence, we introduce another condition which is rather
conservative but easy to be checked.

\begin{definition}[Strict Excitation and Exploitation (SE\tsup{2})]
	A signal $\phi: \MB{R}_{+} \to \MB{R}^n$ is said to be \textit{strictly
	exciting and exploited (SE\tsup{2})} if there exist positive
	scalar $\alpha_1$, $\alpha_2$, $t_1$ and a positive-valued function $d :
	\MB{R}_{+} \times \MB{R}_{+} \to \MB{R}_{+}$ such that
	\begin{equation}\label{eq:SE2}
		\alpha_1 I \le \int_{0}^t d(t, \tau) \phi(\tau) \phi^T(\tau) \dd{\tau}
		\le \alpha_2 I,
	\end{equation}
	for all $t \ge t_1$.
\end{definition}

For convenience, let
\begin{equation}\label{eq:information matrix}
	\MC{M}(t) = \int_0^t d(t, \tau) \phi(\tau) \phi^T(\tau) \dd{\tau}
\end{equation}
denoting the information matrix which expands the notion introduced
in~\cite{cho_composite_2018}.

Note that $\MC{M}(t)$ consists of the known $d(t, \tau)$ and
the unlimited memory storing $\phi(\tau)$ for all $0 \le \tau \le t$, from
which the SE\tsup{2} condition can be easily assessed, by which the exponential
stability of~\eqref{eq:estimation error system memory} is guaranteed.

\begin{lemma}\label{lem:SE2 then PE2}
	If a signal is SE\tsup{2}, then the signal is PE\tsup{2}
\end{lemma}

\begin{proof}
	Since $d \ge 0$, $0 \le \MC{M}(t) \le \alpha_2 I$, for all $0 \le t <
	t_1$. Let us take $\delta = t_1 + \alpha_3$ in~\eqref{eq:PE2}, for some
	$\alpha_3 > 0$. If $0 \le t_0 < t_1$,
	\begin{equation*}
		0 \le \int_{t_0}^{t_1} {\MC{M}(s)} \dd{s} \le \alpha_2 (t_1 - t_0) I.
	\end{equation*}
	and
	\begin{equation*}
		\alpha_1 \alpha_3 I \le \int_{t_1}^{t_0 + \delta} {\MC{M}(s)} \dd{s} \le
		\alpha_2 (t_0 + \alpha_3) I,
	\end{equation*}
	which implies that
	\begin{equation*}
		\alpha_1 \alpha_3 I \le \int_{t_0}^{t_0 + \delta} {\MC{M}(s)} \dd{s} \le
		\alpha_2 \delta I.
	\end{equation*}
	If $t_0 \ge t_1$,
	\begin{equation*}
		\alpha_1 \delta I \le \int_{t_0}^{t_0 + \delta} {\MC{M}(s)} \dd{s} \le
		\alpha_2 \delta I.
	\end{equation*}
	Hence, letting $\beta_1 = \alpha_1 \alpha_3$ and $\beta_2 = \alpha_2 \delta$
	yields~\eqref{eq:PE2} for all $t_0 \ge 0$, which completes the proof.
\end{proof}

With Lemma~\ref{lem:SE2 then PE2} and Theorem~\ref{thm:PE2 then GES}, we can
conclude that if a signal $\phi$ is SE\tsup{2}, then the
system~\eqref{eq:estimation error system memory} is globally exponentially
stable.

The only thing left is how to determine the distribution $d(t, \tau)$ to
satisfy the SE\tsup{2}.

\begin{example}[The Concurrent-Learning Estimators]
	Let $d(t, \tau)$ be
	\begin{equation*}
		d(t, \tau) = \gamma \sum_{s \in \MC{T}(t)} \delta_D(s - \tau),
	\end{equation*}
	where $\MC{T}(t)$ is a set of time instances at time $t$ corresponding to the
	history stack defined in~\cite{chowdhary_exponential_2014}. Then it becomes
	the concurrent-learning estimator.

	Briefly, $t$ is appended to $\MC{T}(t)$ initialized with an empty set, when
	the consequence set increases the rank of the following positive
	semi-definite matrix
	\begin{equation*}
		\int_0^t d(t, \tau) \phi(\tau) \phi^T(\tau) \dd{\tau} = \gamma \sum_{s \in
		\MC{T}(t)} \phi(s) \phi^T(s),
	\end{equation*}
	or the smallest singular value of the matrix is increased.  This algorithm
	uses first-in-first-out (FIFO) update if $n(\MC{T}(t)) = N$ for some $N \ge
	n$ to consider the limited memory size and computation time.
\end{example}

Note that concurrent-learning estimators update its estimation only by the data
measured at particular time instances stored in the history stack. This
algorithm uses the data sparsely, which implies that the data wasted is much
more than used. It may cause two problems: an overfitting, which is vulnerable
to the noise, and a degraded performance for (slowly) varying parameters.

To remedy these problems, there have been studies 

\begin{example}[Methods in~\cite{cho_composite_2018}
	and~\cite{pan_composite_2018}]
\end{example}

what we have to do is to properly distribute the data using $d(t,
\tau)$.


\section{Online Estimation}
% ===========================

In this section, we propose an online estimator that exploits the past data
while satisfying pre-defined eigenvalue conditions. 

Consider the following hybrid update law for $\MC{T}$ being a subset of
$\MB{R}_{+}$.
\begin{equation}\label{eq:hybrid update law}
	\dot{W}(t) = - \Phi(t) W(t) + \Psi(t), \quad t \in \MB{R}_{+},
\end{equation}
where the discrete update part is given by
\begin{subequations}\label{eq:discrete update part}
	\begin{gather}
		\dot{\Phi}(t) = 0, \quad \dot{\Psi}(t) = 0, \\
		\intertext{for $t \in \MB{R}_{+} \setminus \MC{T}$, and}
		\Phi^{+} (t) = r(t) \Phi(t) + s(t) \phi(t) \phi^T(t), \\
		\Psi^{+} (t) = r(t) \Psi(t) + s(t) \phi(t) y^T(t),
	\end{gather}
\end{subequations}
for $t \in \MC{T}$ with real valued functions $r(t) > 0$ and $s(t)$ which will
be defined later. The initial conditions are $\Phi(0) = 0$, $\Psi(0) = 0$, and
$W(0) = W_0$ with proper dimensions.

Let $\MC{T}$ be an increasing sequence of real numbers as $\MC{T} \coloneqq
\{t_k\}$ for $k = 0, 1, \dotsc$, where $ t_0 = 0 < t_1 < t_2 < \cdots $.
Then, the discrete update part~\eqref{eq:discrete update part} implies that
\begin{subequations}\label{eq:summation of discrete update part}
	\begin{align}
		\Phi(t) &= \sum_{i=0}^{k} \qty(\prod_{j=i+1}^{k} r(t_j)) s(t_i) \phi(t_i)
		\phi^T(t_i), \\
		\Psi(t) &= \sum_{i=0}^{k} \qty(\prod_{j=i+1}^{k} r(t_j)) s(t_i) \phi(t_i)
		y^T(t_i),
	\end{align}
\end{subequations}
where $t_k \le t < t_{k+1}$.

\begin{assumption}\label{assumption:discrete bounded eigenvalues}
	A signal $\phi$ satisfies that for some $r$, $s$ and $\MC{T}$ which
	construct $\Phi$ in~\eqref{eq:summation of discrete update part}, there
	exist positive scalars $t_b \in \MC{T}$, $\underline{\lambda}$ and
	$\overline{\lambda} \ge \underline{\lambda}$ such that
	\begin{equation}
		0 < \underline{\lambda} \le \lambda_i\qty(\Phi(t)) \le \overline{\lambda},
	\end{equation}
	for all $t \ge t_b$ and $i = 0, \dotsc, n$.
\end{assumption}

\begin{proposition}
	If a signal $\phi$ satisfies Assumption~\ref{assumption:discrete bounded
	eigenvalues}, then the system~\eqref{eq:hybrid update law} is globally
	exponentially stable.
\end{proposition}

\begin{proof}
	By using $r$, $s$ and $\MC{T}$ satisfying
	Assumption~\ref{assumption:discrete bounded eigenvalues}, let the
	distribution $d(t, \tau)$ be
	\begin{equation*}
		d(t, \tau) = \sum_{i=0}^k \prod_{j=i+1}^k r(t_j) s(t_i) \delta(\tau - t_i),
	\end{equation*}
	for $k$ such that $t_k \le t < t_{k+1}$. Then, the information matrix
	$\MC{M}(t)$ in equation~\eqref{eq:information matrix} satisfies
	\begin{equation*}
		0 < \underline{\lambda} I \le \MC{M}(t) = \Phi(t) \le \overline{\lambda} I,
	\end{equation*}
	for all $t \ge t_b$. This implies that the signal $\phi$ is
	SE\textsuperscript{2}, which completes the proof.
\end{proof}

Consider the following distribution.
\begin{equation}\label{eq:proposed distribution}
	d(t, \tau) = \sigma(\tau) \exp( -\int_\tau^t \rho(s) \dd{s} ),
\end{equation}
where the real-valued functions $\rho, \sigma : \MB{R}_{+} \to \MB{R}$ are
piecewise-continuous with the finite number of discontinuities.

% 

With the proposed distribution~\eqref{eq:proposed distribution}, the
information matrix $\MC{M}(t)$ satisfies the following differential equation.
\begin{equation}
	\dot{\MC{M}}(t) = - \rho(t) \MC{M}(t) + \sigma(t) \phi(t) \phi^T(t), \quad
	\MC{M}(0) = 0.
\end{equation}



\subsection{Convergence Rate}
%============================

There have been lots of work 






%======================================
\section{Eigenvalue Bounding Algorithm}
%======================================

We have seen that 






% \begin{equation*}
% 	\dot{\lambda}_i = x_i^T \dot{\MC{M}} x_i = - \rho \lambda_i + \sigma
% 	\norm{\phi}^2 w_i^2,
% \end{equation*}
% where $w_i = {\abs{\phi^T v_i}}/{\norm{\phi}}$. Since $V \coloneqq [v_1,
% \dotsc, v_n]$ is orthonormal, $\norm{w}^2 = 1$.

% Let a candidate Lyapunov function be
% \begin{equation*}
% 	\MC{V} = \frac{1}{2} \norm*{\tilde{\lambda}}^2,
% \end{equation*}
% where $\tilde{\lambda} = \lambda_0 - \lambda$, and the time derivative is
% \begin{equation*}
% 	\dot{\MC{V}} = - \rho \norm*{\tilde{\lambda}}^2 + \rho \tilde{\lambda}^T
% 	\lambda_0 - \sigma \norm{\phi}^2 w^T \tilde{\Lambda} w.
% \end{equation*}
% If we take
% \begin{equation*}
% 	\sigma = \frac{\rho \tilde{\lambda}^T \lambda_0}{\norm{\phi}^2
% 	\tilde{\lambda}^T w} + \frac{\kappa \tilde{\lambda}^T w}{\norm{\phi}^2} \ge 0
% \end{equation*}

% \begin{equation*}
% 	\sigma = \frac{1}{\norm{\phi}^2} \qty(k_1 \qty(\norm{\diag(\lambda_0)^{-1}
% 	\tilde{\lambda}}_\infty - 1) + k_2 \norm*{\tilde{\lambda}}^2)
% \end{equation*}

% \begin{align*}
% 	\dot{\MC{V}} &= - \rho \norm*{\tilde{\lambda}}^2 + \rho \tilde{\lambda}^T
% 	\lambda_0 - k_1 \qty(\norm{\diag(\lambda_0)^{-1}
% 	\tilde{\lambda}}_\infty - 1) w^T \tilde{\Lambda} w - k_2
% 	\norm*{\tilde{\lambda}}^2 w^T \tilde{\Lambda} w \\
% 	&\le - \rho \norm*{\tilde{\lambda}}^2 + \rho \tilde{\lambda}^T
% 	\lambda_0 - k_1 \qty(\norm{\diag(\lambda_0)^{-1}
% 	\tilde{\lambda}}_\infty - 1) w^T \tilde{\Lambda} w - k_2
% 	\norm*{\tilde{\lambda}}^2 w^T \tilde{\Lambda} w
% \end{align*}

% \begin{align*}
% 	\dot{\MC{V}} &= - \frac{\sigma \norm{\phi}^2 w_n^2 - f_n}{\lambda_n}
% 	\norm{\lambda}^2 + \sigma \norm{\phi}^2 \sum \lambda_i w_i^2 \\
% 	&\ge \sigma \norm{\phi}^2 \norm{\lambda}^2 \qty(-\frac{w_n^2}{\lambda_n} +
% 	\sum \frac{\lambda_i w_i^2}{\norm{\lambda}^2}) + \frac{f_n}{\lambda_n}
% 	\norm{\lambda}^2
% \end{align*}




\bibliographystyle{IEEEtran}
\bibliography{IEEEabrv,manuscript}

\end{document}

