\documentclass[nobib]{my-handout}

\newtheorem{lemma}{Lemma}
\newtheorem{corollary}{Corollary}

\begin{document}
\maketitle

\section{Purpose}

\begin{itemize}
\item
  To suggest a data-efficient adaptive control algorithm with stability
  proof.
\item
  To consider time-varying stochastic uncertainties like wind velocity.
\item
  Relieve the persistent / interval excitation condition by exploiting
  the non-single-step algorithms.
\end{itemize}

\section{Related Works}

\begin{itemize}
	\item Yongping Pan~\cite{pan_composite_2016,pan_composite_2018}
		\begin{itemize}
			\item
				Composite learning control
		\end{itemize}
	\item Girish
		Chowdhary~\cite{chowdhary_concurrent_2013-1,kamalapurkar_concurrent_2017}
		\begin{itemize}
			\item
				Concurrent learning
		\end{itemize}
	\item A Review for computational optimization
		methods~\cite{bottou_optimization_2018}
\end{itemize}

\section{Parameter Estimation}

Consider the system \[
\dot{x}(t) = A x(t) + B_r c(t) + B \Lambda \qty(u(t) + W_0^T \phi_0(t, x)),
\] and the reference system \[
\dot{x}_r(t) = A x_r(t) + B_r c(t),
\] with the system matrix \(A\) being Hurwitz. Then, we have the
following error dynamics \[
\dot{e} = A e(t) + B \Lambda \qty(u(t) + W^T \phi(t, x, c)).
\] The unknowns are a diagonal matrix \(\Lambda\) possessing positive
elements, and a parameter matrix \(W\).

Integrating both side of the equation yields \[
e(t) - e(t-\delta) = A \int_{t-\delta}^t e(\tau) \dd{\tau} - B \Lambda v_1(t) +
B \Lambda W^T v_2(t)
\] with \[
\begin{aligned}
    v_1(t) &= \int_{t-\delta}^t u(\tau) \dd{\tau}, \\
    v_2(t) &= \int_{t-\delta}^t \phi(\cdot) \dd{\tau}, \\
\end{aligned}
\] with \(\delta > 0\), and let \[
y(t) = e(t) - e(t - \delta) - A \int_{t - \delta}^{t} e(\tau) \dd{\tau}.
\] Then, for a sequence \(\{t_n\}\) such that \(t_n > \delta\), a
dataset \(\MC{D}\) of which the input is \((v_1(t_k), v_2(t_k))\) and
the output is \(y(t_k)\) for \(k=0,1,\dotsc\) is generated.

We can define a regressor for \(\MC{D}\) as follows
\begin{equation}
	\begin{aligned}
		\hat{y}(t) &= - B \hat{\Lambda}(t) v_1(t) + B \hat{V}(t) v_2(t),\\
		&= \bmqty{ - v_1^T \otimes B & v_2^T \otimes B}
		\bmqty{\mathrm{vec}(\hat{\Lambda}) \\ \mathrm{vec}(\hat{V})}
	\end{aligned},
\end{equation}
where \(\hat{V} = \hat{\Lambda} \hat{W}^T\). To formulate the problem
as a stochastic gradient problem, we define an error as \[
\epsilon(t) = \norm{\hat{y}(t) - y(t)}^2.
\] Then, we have \[
\dot{\hat{\Lambda}} = \gamma_1 \mathrm{diag}\qty(v_1) \mathrm{diag}\qty(B^T
(\hat{y} - y)),
\] and \[
\begin{aligned}
    \dot{\hat{V}} &=  - \gamma_2 \qty(v_2 \otimes B^T) (\hat{y} - y) \\
    &= - \gamma_2 B^T (\hat{y} - y) v_2^T.
\end{aligned}
\] Since
\(\dot{\hat{V}} = \dot{\hat{\Lambda}} \hat{W}^T + \hat{\Lambda} \dot{\hat{W}}^T\),
\[
\dot{\hat{W}} = - \gamma_1 \hat{W} \mathrm{diag}\qty(v_1) \mathrm{diag}\qty(B^T
(\hat{y} - y)) \hat{\Lambda}^{-1} - \gamma_2 v_2 (\hat{y} - y)^T B
\hat{\Lambda}^{-1}.
\]

\section{Review of Parameter Estimation Techniques}

\begin{itemize}
	\item
		\textbf{Time-Varying Parameter Identification
		Algorithm}~\cite{rios_time-varying_2017}
\end{itemize}

Consider \[
\begin{aligned}
    \dv{\theta(t)}{t} &= \Theta(wt), \\
    y(t) &= \Gamma^T(wt) \theta(t) + \varepsilon(t).
\end{aligned}
\] In order to estimate the parameter, the following update law was
introduced. \[
\dot{\hat{\theta}}(t) = - K \Gamma(wt) \left\lceil \Gamma^T(wt) \hat{\theta}(t)
- y(t) \right\rfloor^\gamma,
\] where
\(\lceil \cdot \rfloor^\gamma \coloneqq \abs{\cdot}^\gamma \mathrm{sign}(\cdot)\).

\begin{itemize}
\item
	\textbf{Stochastic Gradient Descent in Continuous
	Time}\cite{sirignano_stochastic_2017}
\end{itemize}

Consider \[
\dd{X_t} = f^\ast (X_t) \dd{t} + \sigma \dd{W_t},
\] where the goal is to statistically estimate a model \(f(x, \theta)\)
for \(f^\ast\), where \(\theta \in \MB{R}^n\).


\begin{shade}[Note]
	We want to devise an exponential or similarly fast convergence of parameter
	estimation for dynamical system with continuous analysis.
\end{shade}


\section{Recursive Least Square}


\section{Concurrent Learning}

Concurrent learning schemes use a fixed dataset to update the estimation for a
while, so that it can guarantee the exponential convergence of the parameter
estimation.

The (exponential) convergence rate to the true value of the unknown parameter
is proportional to the minimum eigenvalue of constructed dataset.  Hence, the
dataset is updated when a new data increases the minimum eigenvalue.

However, the major drawback of concurrent learning is duplicated usages of data
points, which is vulnerable to noises and changes of the parameter.

What this algorithm does can be viewed as assigning a high weight to the
dataset maximizing the minimum eigenvalue. From this perspective, there arise
two questions.
\begin{enumerate}
	\item Is there a decent way to maximize the minimum eigenvalue of the dataset
		in continuous-time setup?
	\item Does it also guarantee the exponential convergence?
\end{enumerate}


\section{LMI Formulation}

If we have a history of regression vectors for each time, an weighted integral
can be formulated as
\[ \Phi_T^a(t) = \int_{t-T}^t a(t-\tau) \phi(\tau) \phi^T(\tau) \dd{\tau}, \]
where $a : \MB{R}_{+} \to \MB{R}$ satisfies the following condition
\[ \int_0^T a(t) \dd{t} = b, \]
with $b$ being a positive constant.

Now, the problem we consider can be stated as follows.
\begin{equation*}
	\begin{aligned}
		\operatorname*{maximize}_{a(\cdot)}\quad & \lambda_{\min} (\Phi_T^a(t)) \\
		\text{subject to}\quad & \int_0^T a(t) \dd{t} = b
	\end{aligned}
\end{equation*}

As $\Phi_T^a(\cdot)$ is the sum of symmetric matrices, it can be equivalently
formulated as
\begin{equation*}
	\begin{aligned}
		\operatorname*{maximize}_{a(\cdot),\ s}\quad & s \\
		\text{subject to}\quad & \Phi_T^a(t) - s \cdot I \succeq 0\\
		& \int_0^T a(t) \dd{t} = b
	\end{aligned}
\end{equation*}


\section{SDP Formulation}

The above problem can be viewed as an SDP problem.
\begin{equation*}
	\begin{aligned}
		\operatorname*{minimize}_{x}\quad & c^T x \\
		\text{subject to}\quad & \sum_{i=1}^N x_i \Phi_i + s \cdot (-I) \succeq 0\\
		& \int_0^T a(t) \dd{t} = b
	\end{aligned}
\end{equation*}
where
\begin{equation*}
	\begin{aligned}
		c &= [0, \dotsc, 0, -1]^T \\
		x &= [a_1, \dotsc, a_N, s]^T \\
	\end{aligned}
\end{equation*}


\section{Appendix}

\begin{lemma}[Some facts of matrix algebra]
	All matrices we discuss here are over the real numbers.
	\begin{enumerate}
		\item (See \cite{ulukok_matrix_2010}) If $A$ and $B$ are positive
			semi-definite matrices, then, \[ 0 \le \tr(AB) \le \tr(A) \tr(B). \] 
	\end{enumerate}
\end{lemma}

\begin{lemma}
	Let $A$ be a symmetric matrix. Let $\lambda_{\min}$, $\lambda_{\max}$
	denote respectively the smallest and largest eigenvalue of $A$. Then,
	\[ \lambda_{\min} \cdot I \preceq A \preceq \lambda_{\max} \cdot I \]
\end{lemma}

\begin{proof}
	We will show only the first inequality, which is equivalent to $A -
	\lambda_{\min} \cdot I \succeq 0$. Let $\lambda_1, \dotsc, \lambda_d$ be the
	eigenvalues of $A$, then, the eigenvalues of $A - \lambda_{\min} \cdot I$ are
	\[ {\lambda_1 - \lambda_{\min}, \dotsc, \lambda_d - \lambda_{\min}}. \]
	Observe that
	\[ \min_j(\lambda_j - \lambda_{\min}) = 0, \]
	which completes the proof.
\end{proof}

\begin{lemma}\label{lemma:tr_AB_le_AC}
	Let A, B and C be symmetric $d \times d$ matrices satisfying $A \succeq 0$
	and $B \preceq C$. Then
	\[ \tr(AB) \le \tr(AC) \]
\end{lemma}

\begin{proof}
	Let $A = \sum_{i=1}^d \lambda_i u_i u_i^T$, where $\{u_1, \dotsc, u_d\}$ is an
	orthonormal basis consisting of eigenvectors of $A$, and $\lambda_i$ is the
	eigenvalue corresponding to $u_i$. Since $A \succeq 0$, we can set $v_i =
	\sqrt{\lambda_i} u_i$ and write $A = \sum_{i=1}^d v_i v_i^T$. Then,
	\begin{equation*}
		\begin{aligned}
			\tr(AB) &= \tr(\sum_i v_i v_i^T B) = \sum_i \tr(v_i^T B v_i) \\
							&\le \sum_i \tr(v_i^T C v_i) = \tr(AC).
		\end{aligned}
	\end{equation*}
\end{proof}

\begin{corollary}
	Let $B = \sum_{i=1}^d v_i v_i^T$, and $\lambda_{\min}(B)$ be the smallest
	eigenvalue of $B$. Then,
	\[ \tr(A^T B A) \ge \lambda_{\min}(B) \tr(A^T A). \]
\end{corollary}

\begin{proof}
	Since $B \succeq \lambda_{\min}(B) \cdot I$, and $A A^T$ is symmetric, we can
	apply Lemma \ref{lemma:tr_AB_le_AC} as
	\[ \tr(A^T B A) = \tr(B A A^T) \ge \tr(\lambda_{\min}(B) A A^T) =
	\lambda_{\min}(B) \tr(A^T A). \]
\end{proof}



\bibliographystyle{plain}
\bibliography{memo.bib}
\end{document}
